{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Chunking##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP This/DT)\n",
      "  has/VBZ\n",
      "  increased/VBN\n",
      "  (NP the/DT risk/NN)\n",
      "  of/IN\n",
      "  (NP the/DT government/NN)\n",
      "  being/VBG\n",
      "  forced/VBN\n",
      "  to/TO\n",
      "  increase/VB\n",
      "  (NP base/NN rates/NNS)\n",
      "  to/TO\n",
      "  (NP 16/CD %/NN)\n",
      "  from/IN\n",
      "  (NP their/PRP$ current/JJ 15/CD %/NN level/NN)\n",
      "  to/TO\n",
      "  defend/VB\n",
      "  (NP the/DT pound/NN)\n",
      "  ,/,\n",
      "  (NP economists/NNS)\n",
      "  and/CC\n",
      "  (NP foreign/JJ exchange/NN market/NN analysts/NNS)\n",
      "  say/VBP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "trees = nltk.corpus.conll2000.chunked_sents('train.txt', chunk_types='NP')\n",
    "print(trees[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  This/DT\n",
      "  (VP has/VBZ increased/VBN)\n",
      "  the/DT\n",
      "  risk/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  government/NN\n",
      "  (VP being/VBG forced/VBN to/TO increase/VB)\n",
      "  base/NN\n",
      "  rates/NNS\n",
      "  to/TO\n",
      "  16/CD\n",
      "  %/NN\n",
      "  from/IN\n",
      "  their/PRP$\n",
      "  current/JJ\n",
      "  15/CD\n",
      "  %/NN\n",
      "  level/NN\n",
      "  (VP to/TO defend/VB)\n",
      "  the/DT\n",
      "  pound/NN\n",
      "  ,/,\n",
      "  economists/NNS\n",
      "  and/CC\n",
      "  foreign/JJ\n",
      "  exchange/NN\n",
      "  market/NN\n",
      "  analysts/NNS\n",
      "  (VP say/VBP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "trees = nltk.corpus.conll2000.chunked_sents('train.txt', chunk_types='VP')\n",
    "print(trees[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate tracing by testing two chunkers with different rule ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cp1 = nltk.RegexpParser(r'''\n",
    "NP: {<DT><JJ.*><NN.*>} #Chunk det+adj+noun\n",
    "    {<DT|NN.*>+}      #Chunk sequences of DT and noun\n",
    "    ''')\n",
    "cp2 = nltk.RegexpParser(r'''\n",
    "NP:   {<DT|NN.*>+}      #Chunk sequences of DT and noun\n",
    "     {<DT><JJ.*><NN.*>} #Chunk det+adj+noun\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sample sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('enchantress', 'NN'),\n",
       " ('clutched', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('hair', 'NN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(nltk.word_tokenize(\"The enchantress clutched the beautiful hair\"))\n",
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Input:\n",
      " <DT>  <NN>  <VBD>  <DT>  <JJ>  <NN> \n",
      "# Chunk det+adj+noun:\n",
      " <DT>  <NN>  <VBD> {<DT>  <JJ>  <NN>}\n",
      "# Chunk sequences of DT and noun:\n",
      "{<DT>  <NN>} <VBD> {<DT>  <JJ>  <NN>}\n",
      "(S\n",
      "  (NP The/DT enchantress/NN)\n",
      "  clutched/VBD\n",
      "  (NP the/DT beautiful/JJ hair/NN))\n"
     ]
    }
   ],
   "source": [
    "print(cp1.parse(tagged_tokens, trace=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Input:\n",
      " <DT>  <NN>  <VBD>  <DT>  <JJ>  <NN> \n",
      "# Chunk sequences of DT and noun:\n",
      "{<DT>  <NN>} <VBD> {<DT>} <JJ> {<NN>}\n",
      "# Chunk det+adj+noun:\n",
      "{<DT>  <NN>} <VBD> {<DT>} <JJ> {<NN>}\n",
      "(S\n",
      "  (NP The/DT enchantress/NN)\n",
      "  clutched/VBD\n",
      "  (NP the/DT)\n",
      "  beautiful/JJ\n",
      "  (NP hair/NN))\n"
     ]
    }
   ],
   "source": [
    "print(cp2.parse(tagged_tokens, trace=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Collocations##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A.M.', 'starring'),\n",
       " ('A40-AjK', 'Mercedes'),\n",
       " ('Air', 'Force'),\n",
       " ('Akita', 'prefectures'),\n",
       " ('Appian', 'Way'),\n",
       " ('Arc', 'de'),\n",
       " ('Armed', 'Forces'),\n",
       " ('Ash', 'Road'),\n",
       " ('Auto', 'Company'),\n",
       " (\"Best's\", 'Liliputian')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "import string, random\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    nltk.corpus.brown.words(categories=\"romance\"))\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "finder.apply_word_filter(lambda w: w[0] in string.punctuation)\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1,257,700', 'non-farm'),\n",
       " ('100-yard', 'dash'),\n",
       " ('1044', 'Chestnut'),\n",
       " ('11-7', 'collapse'),\n",
       " ('1200', 'Larimer'),\n",
       " ('13-5', 'barrage'),\n",
       " ('165-unit', 'Harbor'),\n",
       " ('1671', 'Nakoma'),\n",
       " ('182', 'scholastics'),\n",
       " ('2-and-2', 'pitches'),\n",
       " ('21-year', 'typhoon'),\n",
       " ('22-12', 'upset'),\n",
       " ('220-yard', 'par-3'),\n",
       " ('2269', 'Serra'),\n",
       " ('255', 'Brook'),\n",
       " ('2705', 'Fitzhugh'),\n",
       " ('2731', 'Pall'),\n",
       " ('3-year-old', 'filly'),\n",
       " ('325', 'crippled'),\n",
       " ('330', 'Woodland')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = BigramCollocationFinder.from_words(\n",
    "    nltk.corpus.brown.words(categories=\"news\"))\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "finder.apply_word_filter(lambda w: w[0] in string.punctuation)\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words)\n",
    "finder.nbest(bigram_measures.chi_sq, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
