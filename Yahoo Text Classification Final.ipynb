{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import punctuation\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from sklearn.feature_extraction.text import L\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading & Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"newtrain.csv\", low_memory=False)\n",
    "testing_data = pd.read_csv(\"newtest.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomize data\n",
    "random_index = np.random.permutation(training_data.index)\n",
    "training_data = training_data.ix[random_index]\n",
    "training_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>if light travels faster than sound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>what books would you like to see made into a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>what type of mpenis do u think is the best?cut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>how do i delete search words?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>what is philtrum?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               Text\n",
       "0         7                 if light travels faster than sound\n",
       "1         3  what books would you like to see made into a m...\n",
       "2         6  what type of mpenis do u think is the best?cut...\n",
       "3         1                      how do i delete search words?\n",
       "4         6                                  what is philtrum?"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head() # peek at training data to make sure nothing went horribly wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "### Combining Tf-idf & Count Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# custom stopwords\n",
    "my_stopwords = ['yahoo', 'best', 'know', 'what', 'how', \"what's\", 'why', \"i'm\", \"xa\", \"would\",'and', 'or',\n",
    "                \"anyone\", 'someone', 'help', 'think', 'find', 'want', 'one', 'the', 'to', 'is', 'of', 'so', \n",
    "                'in', 'do', 'you', 'can', 'it', 'for', 'my', 'on', 'are', 'have', 'is the', 'that', 'with', \n",
    "                'if', 'me', 'does', 'be', 'there', 'was', 'this', 'an', 'but', 'about', 'should', 'any', \n",
    "                'am', 'has', 'just', 'anybody', 'somebody', 'had', 'not', 'some', 'except', 'these', 'those', \n",
    "                'could', 'over', 'will']\n",
    "# single-letter words\n",
    "stops = list(string.ascii_lowercase) + my_stopwords\n",
    "\n",
    "#Using both tfidf and count vectorizers and combining them\n",
    "tfvec = TfidfVectorizer(ngram_range=(1, 3), min_df = 2, stop_words = stops, token_pattern = r'\\b\\w+\\b')\n",
    "countVec = CountVectorizer(ngram_range=(1, 3), min_df = 2, stop_words = stops, \n",
    "                           token_pattern = r'\\b\\w+\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Adding more training data just for Category One\n",
    "Have experimented with adding more training data for category one vs. adding training data for all other categories since confusion matrix indidates category one is where most error occur. It only improved the accuracy by a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoryOne = training_data[training_data.Category == 1]\n",
    "training_data = training_data.append(categoryOne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Apply Vectorizers and combine the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1874, 3463)"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set for tfidf\n",
    "arr_train_feature_sparse = tfvec.fit_transform(training_data.Text)\n",
    "arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "# testing set for tfidf\n",
    "arr_test_feature_sparse = tfvec.transform(testing_data.Text)\n",
    "arr_test_feature = arr_test_feature_sparse.toarray()\n",
    "# training set for count\n",
    "arr_train_feature_sparse_Count = countVec.fit_transform(training_data.Text)\n",
    "arr_train_feature_Count = arr_train_feature_sparse_Count.toarray()\n",
    "arr_train_feature_Count.shape\n",
    "# testing set for count\n",
    "arr_test_feature_sparse_Count = countVec.transform(testing_data.Text)\n",
    "arr_test_feature_Count = arr_test_feature_sparse_Count.toarray()\n",
    "arr_test_feature_Count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1874, 6926)"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining both vectorizers\n",
    "arr_train_feature = np.hstack((arr_train_feature, arr_train_feature_Count))\n",
    "arr_test_feature = np.hstack((arr_test_feature, arr_test_feature_Count))\n",
    "arr_test_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Feature Experiments (We didn't end up using these features)\n",
    "### Feature application method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accepts a feature function and applies it to\n",
    "# the training and testing sets;\n",
    "# returns train_feats, test_feats\n",
    "def apply_feature(fn):\n",
    "    # for the training set\n",
    "    feats = []\n",
    "    for i in training_data.Text:\n",
    "        feats.append(fn(i))\n",
    "    train_feats = np.array( pd.DataFrame(feats) )\n",
    "    \n",
    "    # for the testing set\n",
    "    feats_test = []\n",
    "    for i in testing_data.Text:\n",
    "        feats_test.append(fn(i))\n",
    "    test_feats = np.array( pd.DataFrame(feats_test) )\n",
    "    \n",
    "    return train_feats, test_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words in question *(not used)*\n",
    "tanks nb accuracy (did not end up using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def countLength(text):\n",
    "    result = text.split()\n",
    "    return(str(len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of non-alpha characters in a question *(not used)*\n",
    "kills nb accuracy (did not end up using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nonNumeric(string):\n",
    "    count = 0\n",
    "    for char in string:\n",
    "        if not char.isalpha():\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words longer than 9 characters in a question *(not used)*\n",
    "maybe for logical regression, but not nb (did not end up using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def longWord(string):\n",
    "    count = 0\n",
    "    for word in word_tokenize(string):\n",
    "        if len(word) > 9:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of punctuation marks in a question *(not used)*\n",
    "not effective (did not end up using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def puncts(quest):\n",
    "    punctuation_marks = [char for char in list(quest) if char in set(string.punctuation)]\n",
    "    return len(punctuation_marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of longest word in a question *(not used)*\n",
    "decreased accuracy for nb, same for lr (did not end up using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def longest_word_length(quest):\n",
    "    return len(max(word_tokenize(quest), key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Number of verbs in a question, with stopwords removed *(not used)*\n",
    "questionably effective (did not end up using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gotta build a tagger to get parts of speech\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('X')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "# hopefully these brown categories will be good enough; I spot checked some results\n",
    "brown_tagged_sents = brown.tagged_sents(tagset='universal', \n",
    "                                        categories=['reviews', 'news', 'romance', 'adventure', 'fiction', \n",
    "                                                    'hobbies', 'religion', 'science fiction', 'novel'] )\n",
    "tagger = build_backoff_tagger(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def num_verbs(quest):\n",
    "    sents = sent_tokenizer.tokenize(quest)\n",
    "    verb_count = 0\n",
    "    for sent in [tagger.tag(word_tokenize(s)) for s in sents]:\n",
    "        verb_count = verb_count + len([w[0] for w in sent if w[1]=='VERB' and w[0] not in stopwords.words('english')])\n",
    "    return verb_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Number of adjectives in a question, with stopwords removed *(not used)*\n",
    "nothing or worse :( (did not end up using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def num_adjs(quest):\n",
    "    sents = sent_tokenizer.tokenize(quest)\n",
    "    adj_count = 0\n",
    "    for sent in [tagger.tag(word_tokenize(s)) for s in sents]:\n",
    "        adj_count = adj_count + len([w[0] for w in sent if w[1]=='ADJ' and w[0] not in stopwords.words('english')])\n",
    "    return adj_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Percentage adjectives (without stopwords) in a question\n",
    "eh, doesn't do much, might include this (did not end up using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percent_adjs(quest):\n",
    "    sents = sent_tokenizer.tokenize(quest)\n",
    "    adj_count = 0\n",
    "    word_count = 0\n",
    "    for sent in [tagger.tag(word_tokenize(s)) for s in sents]:\n",
    "        adj_count = adj_count + len([w[0] for w in sent if w[1]=='ADJ'])\n",
    "        word_count = word_count + len(sent)\n",
    "    return adj_count/word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Percentage of verbs (with stopwords) in a question\n",
    "helps a little on nb (did not end up using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percent_verbs(quest):\n",
    "    sents = sent_tokenizer.tokenize(quest)\n",
    "    verb_count = 0\n",
    "    word_count = 0\n",
    "    for sent in [tagger.tag(word_tokenize(s)) for s in sents]:\n",
    "        verb_count = verb_count + len([w[0] for w in sent if w[1]=='VERB'])\n",
    "        word_count = word_count + len(sent)\n",
    "    return verb_count/word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Percentage of nouns (without stopwords) in a question *(not in use)*\n",
    "don't use (did not end up using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percent_nouns(quest):\n",
    "    sents = sent_tokenizer.tokenize(quest)\n",
    "    noun_count = 0\n",
    "    word_count = 0\n",
    "    for sent in [tagger.tag(word_tokenize(s)) for s in sents]:\n",
    "        noun_count = noun_count + len([w[0] for w in sent if w[1]=='NOUN'])\n",
    "        word_count = word_count + len(sent)\n",
    "    return noun_count/word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Apply Selected Features Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feats, test_feats = apply_feature(percent_verbs)\n",
    "\n",
    "# add new features to our feature arrays\n",
    "arr_train_feature = np.hstack((arr_train_feature, train_feats))\n",
    "arr_test_feature = np.hstack((arr_test_feature, test_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature View/Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2698, 6926)"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_train_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossvalidation(func, arr_train_feature, predictions_train, cv=10):\n",
    "    scores = cross_validation.cross_val_score(func, arr_train_feature, predictions_train, cv=cv)\n",
    "    print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.563 (+/- 0.044)\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb_model = nb.fit(arr_train_feature, training_data.Category)\n",
    "nb_predictions = nb_model.predict(arr_test_feature)\n",
    "#nb_predictions[:1]\n",
    "crossvalidation(nb_model, arr_train_feature, training_data.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.537 (+/- 0.046)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(arr_train_feature, training_data.Category)\n",
    "log_predictions = logreg_model.predict(arr_test_feature)\n",
    "crossvalidation(logreg_model, arr_train_feature, training_data.Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_id = testing_data['Id']\n",
    "_category = nb_predictions\n",
    "final_d = {\"ID\":_id}\n",
    "final_df = pd.DataFrame(data=final_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_d2 = {\"Category\":_category}\n",
    "final_df2 = pd.DataFrame(data=final_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new = pd.concat([final_df, final_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Category\n",
       "0   1         4\n",
       "1   2         4\n",
       "2   3         1\n",
       "3   4         2\n",
       "4   5         2"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new.to_csv(\"result8.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
