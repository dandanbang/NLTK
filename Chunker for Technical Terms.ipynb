{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Syntactic Patterns for Technical Terms ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the Manning and Schuetze chapter, there is a well-known part-of-speech \n",
    "based pattern defined by Justeson and Katz for identifying simple noun phrases \n",
    "that often words well for pulling out keyphrases.\n",
    "\n",
    " Technical Term  T = (A | N)+ (N | C)  | N\n",
    "\n",
    "Below, write a function to  define a chunker using the RegexpParser as illustrated in the NLTK book Chapter 7 section 2.3 *Chunking with Regular Expressions*.  You'll need to revise the grammar rules shown there to match the pattern shown above.  You can be liberal with your definition of what is meant by *N* here.  Also, C refers to cardinal number, which is CD in the brown corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<JJ|NN.*>+<(NN.*|CD)>+<NN.*>?}  \n",
    "\"\"\"\n",
    "\n",
    "def chunker(sent):\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    print (cp.parse(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, write a function to call the chunker, run it on some sentences, and then print out the results for  those sentences.\n",
    "\n",
    "For uniformity, please run it on sentences 100 through 104 from the full tagged brown corpus.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Daniel/NP\n",
      "  personally/RB\n",
      "  led/VBD\n",
      "  the/AT\n",
      "  fight/NN\n",
      "  for/IN\n",
      "  the/AT\n",
      "  measure/NN\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  he/PPS\n",
      "  had/HVD\n",
      "  watered/VBN\n",
      "  down/RP\n",
      "  considerably/RB\n",
      "  since/IN\n",
      "  its/PP$\n",
      "  rejection/NN\n",
      "  by/IN\n",
      "  two/CD\n",
      "  (NP previous/JJ Legislatures/NNS-TL)\n",
      "  ,/,\n",
      "  in/IN\n",
      "  a/AT\n",
      "  (NP public/JJ hearing/NN)\n",
      "  before/IN\n",
      "  the/AT\n",
      "  (NP House/NN-TL Committee/NN-TL)\n",
      "  on/IN-TL\n",
      "  Revenue/NN-TL\n",
      "  and/CC-TL\n",
      "  Taxation/NN-TL\n",
      "  ./.)\n",
      "(S\n",
      "  Under/IN\n",
      "  (NP committee/NN rules/NNS)\n",
      "  ,/,\n",
      "  it/PPS\n",
      "  went/VBD\n",
      "  automatically/RB\n",
      "  to/IN\n",
      "  a/AT\n",
      "  subcommittee/NN\n",
      "  for/IN\n",
      "  one/CD\n",
      "  week/NN\n",
      "  ./.)\n",
      "(S\n",
      "  But/CC\n",
      "  questions/NNS\n",
      "  with/IN\n",
      "  which/WDT\n",
      "  (NP committee/NN members/NNS)\n",
      "  taunted/VBD\n",
      "  bankers/NNS\n",
      "  appearing/VBG\n",
      "  as/CS\n",
      "  witnesses/NNS\n",
      "  left/VBD\n",
      "  little/AP\n",
      "  doubt/NN\n",
      "  that/CS\n",
      "  they/PPSS\n",
      "  will/MD\n",
      "  recommend/VB\n",
      "  passage/NN\n",
      "  of/IN\n",
      "  it/PPO\n",
      "  ./.)\n",
      "(S\n",
      "  Daniel/NP\n",
      "  termed/VBD\n",
      "  ``/``\n",
      "  extremely/RB\n",
      "  conservative/JJ\n",
      "  ''/''\n",
      "  his/PP$\n",
      "  estimate/NN\n",
      "  that/CS\n",
      "  it/PPS\n",
      "  would/MD\n",
      "  produce/VB\n",
      "  17/CD\n",
      "  million/CD\n",
      "  dollars/NNS\n",
      "  to/TO\n",
      "  help/VB\n",
      "  erase/VB\n",
      "  an/AT\n",
      "  anticipated/VBN\n",
      "  deficit/NN\n",
      "  of/IN\n",
      "  63/CD\n",
      "  million/CD\n",
      "  dollars/NNS\n",
      "  at/IN\n",
      "  the/AT\n",
      "  end/NN\n",
      "  of/IN\n",
      "  the/AT\n",
      "  (NP current/JJ fiscal/JJ year/NN)\n",
      "  next/AP\n",
      "  Aug./NP\n",
      "  31/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "brown = nltk.corpus.brown\n",
    "brown_subset = brown.tagged_sents()[100:104]\n",
    "\n",
    "def callChunk(brown_subset):\n",
    "    for sub in brown_subset:\n",
    "        chunker(sub)\n",
    "callChunk(brown_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then extract out the phrases themselves on sentences 100 through 160 using the subtree extraction technique shown in the \n",
    "*Exploring Text Corpora* category.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CHUNK previous/JJ Legislatures/NNS-TL)\n",
      "(CHUNK public/JJ hearing/NN)\n",
      "(CHUNK House/NN-TL Committee/NN-TL)\n",
      "(CHUNK committee/NN rules/NNS)\n",
      "(CHUNK committee/NN members/NNS)\n",
      "(CHUNK current/JJ fiscal/JJ year/NN)\n",
      "(CHUNK escheat/NN law/NN)\n",
      "(CHUNK bank/NN accounts/NNS)\n",
      "(CHUNK personal/JJ property/NN)\n",
      "(CHUNK insurance/NN firms/NNS)\n",
      "(CHUNK pipeline/NN companies/NNS)\n",
      "(CHUNK such/JJ property/NN)\n",
      "(CHUNK state/NN treasurer/NN)\n",
      "(CHUNK escheat/NN law/NN)\n",
      "(CHUNK such/JJ property/NN)\n",
      "(CHUNK Bankers/NNS-TL Association/NN-TL)\n",
      "(CHUNK opposition/NN keynote/NN)\n",
      "(CHUNK contractual/JJ obligations/NNS)\n",
      "(CHUNK bank/NN customers/NNS)\n",
      "(CHUNK taxpayers'/NNS$ pockets/NNS)\n",
      "(CHUNK pipeline/NN companies/NNS)\n",
      "(CHUNK day/NN schools/NNS)\n",
      "(CHUNK special/JJ schooling/NN)\n",
      "(CHUNK deaf/JJ students/NNS)\n",
      "(CHUNK scholastic/JJ age/NN)\n",
      "(CHUNK Education/NN-TL Agency/NN-TL)\n",
      "(CHUNK county-wide/JJ day/NN schools/NNS)\n",
      "(CHUNK deaf/JJ children/NNS)\n",
      "(CHUNK day/NN schools/NNS)\n",
      "(CHUNK day/NN schools/NNS)\n",
      "(CHUNK year's/NN$ capital/NN outlay/NN)\n",
      "(CHUNK day/NN school/NN)\n",
      "(CHUNK state/NN deaf/JJ school/NN)\n",
      "(CHUNK horse/NN race/NN parimutuels/NNS)\n",
      "(CHUNK constitutional/JJ amendment/NN)\n",
      "(CHUNK horse/NN race/NN betting/NN)\n",
      "(CHUNK political/JJ career/NN)\n",
      "(CHUNK House/NN-TL committee/NN)\n",
      "(CHUNK local/JJ option/NN proposal/NN)\n",
      "(CHUNK favorable/JJ report/NN)\n",
      "(CHUNK hard/JJ sledding/NN)\n",
      "(CHUNK State/NN-TL Health/NN-TL Department's/NN$-TL authority/NN)\n",
      "(CHUNK meager/JJ fare/NN)\n",
      "(CHUNK House/NN-TL bills/NNS)\n",
      "(CHUNK school/NN districts/NNS)\n",
      "(CHUNK Navigation/NN-TL District/NN-TL)\n",
      "(CHUNK county/NN-TL Hospital/NN-TL District/NN-TL)\n",
      "(CHUNK special/JJ constitutional/JJ amendment/NN)\n",
      "(CHUNK Gulf/NN-TL Coast/NN-TL district/NN)\n",
      "(CHUNK State/NN-TL Hospital/NN-TL board/NN)\n",
      "(CHUNK tax/NN revision/NN bills/NNS)\n",
      "(CHUNK miscellaneous/JJ excise/NN taxes/NNS)\n",
      "(CHUNK $1,000/NNS fine/NN)\n",
      "(CHUNK Research/NN-TL League/NN-TL)\n",
      "(CHUNK real/JJ estate/NN brokers/NNS)\n",
      "(CHUNK $12/NNS annual/JJ occupation/NN license/NN)\n",
      "(CHUNK Natural/JJ gas/NN public/JJ utility/NN companies/NNS)\n",
      "(CHUNK eminent/JJ domain/NN)\n",
      "(CHUNK underground/JJ storage/NN reservoirs/NNS)\n",
      "(CHUNK Highway/NN-TL Commission/NN-TL)\n",
      "(CHUNK appointment/NN future/JJ vacancies/NNS)\n",
      "(CHUNK costly/JJ special/JJ elections/NNS)\n",
      "(CHUNK lieutenant/NN governor/NN)\n",
      "(CHUNK attorney/NN general/NN)\n",
      "(CHUNK State/NN representatives/NNS)\n",
      "(CHUNK adverse/JJ vote/NN)\n",
      "(CHUNK State/NN-TL Affairs/NNS-TL Committee/NN-TL)\n",
      "(CHUNK poll/NN idea/NN)\n",
      "(CHUNK public/JJ hearing/NN)\n",
      "(CHUNK final/JJ legislative/JJ approval/NN)\n",
      "(CHUNK two-thirds/NNS majorities/NNS)\n",
      "(CHUNK tax/NN proposals/NNS)\n",
      "(CHUNK intelligent/JJ choice/NN)\n",
      "(CHUNK water/NN needs/NNS)\n",
      "(CHUNK big/JJ cities/NNS)\n",
      "(CHUNK water/NN development/NN bill/NN)\n",
      "(CHUNK big/JJ cities/NNS)\n",
      "(CHUNK County/NN-TL hamlet/NN)\n",
      "(CHUNK similar/JJ proposal/NN)\n",
      "(CHUNK maximum/JJ loan/NN)\n",
      "(CHUNK local/JJ water/NN project/NN)\n",
      "(CHUNK large/JJ cities/NNS)\n",
      "(CHUNK water/NN program/NN)\n",
      "(CHUNK local/JJ bonds/NNS)\n",
      "(CHUNK populous/JJ places/NNS)\n",
      "(CHUNK pending/JJ bill/NN)\n"
     ]
    }
   ],
   "source": [
    "brown_subset2 = brown.tagged_sents()[100:160]\n",
    "cp = nltk.RegexpParser('CHUNK: {<JJ|NN.*>+<(NN.*|CD)>+<NN.*>?} ')\n",
    "for sent in brown_subset2:\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'CHUNK': print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify Proper Nouns ##\n",
    "For this next task, write a new version of the chunker, but this time change it in two ways:\n",
    " 1. Make it recognize proper nouns\n",
    " 2. Make it work on your personal text collectionÂ which means that you need to run a tagger over your personal text collection.\n",
    "\n",
    "Note that the second requirements means that you need to run a tagger over your personal text collection before you design the proper noun recognizer.  You can use a pre-trained tagger or train your own on one of the existing tagged collections (brown, conll, or treebank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tagger:** Your code for optionally training tagger, and for definitely running tagger on your personal collection goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.org/stream/SteveJobs/SteveJobs_djvu.txt\" #load the url\n",
    "html = urlopen(url).read() #read the text from the url\n",
    "soup = BeautifulSoup(html) \n",
    "soupText = soup.get_text() #use Beautiful Soup to clean up the HTML mark up\n",
    "soupText.find(\"The Adoption\") #find the beginning of the first chapter\n",
    "soupText.rfind(\"ACKNOWLEDGMENTS\") #find the end of the book\n",
    "text = soupText[26001:1235482] #read the text from the beginning of the chapter to the end of the book\n",
    "sents = sent_tokenizer.tokenize(text) #dividing it up into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences\n",
    "    \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_Steve = tokenize_text(soupText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories=['belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "'humor','mystery', 'religion', 'reviews', 'romance','science_fiction'])\n",
    "\n",
    "train_sents, test_sents = create_data_sets(brown_tagged_sents)\n",
    "\n",
    "ngram_tagger = build_backoff_tagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steve_tagged = []\n",
    "for sent in tokenized_Steve:\n",
    "    steve_tagged.append(ngram_tagger.tag(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunker:** Code for the proper noun chunker goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk(text):\n",
    "    chunklist = []\n",
    "    cp = nltk.RegexpParser('CHUNK: {<NN-.*>+<IN>?}') #My text's proper noun is being tagged as NN-TL..etc\n",
    "    for sub in text:\n",
    "        tree = cp.parse(sub)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'CHUNK':\n",
    "                chunklist.append (str(subtree))\n",
    "    return chunklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the Chunker:** Test your proper noun recognizer on a lot of sentences to see how well it is working.  You might want to add prepositions in order to improve your results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunklist = chunk(steve_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FreqDist Results:** After you have your proper noun recognizer working to your satisfaction, below  run it over your entire collection, feed the results into a FreqDist, and then print out the top 20 proper nouns by frequency.  That code goes here, along with the output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(chunklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(CHUNK Time/NN-TL)', 61),\n",
       " ('(CHUNK Bill/NN-TL)', 52),\n",
       " ('(CHUNK Story/NN-TL)', 44),\n",
       " ('(CHUNK Valley/NN-TL)', 31),\n",
       " ('(CHUNK Rock/NN-TL)', 30),\n",
       " ('(CHUNK Wall/NN-TL Street/NN-TL Journal/NN-TL)', 28),\n",
       " ('(CHUNK Art/NN-TL)', 26),\n",
       " ('(CHUNK Life/NN-TL)', 24),\n",
       " ('(CHUNK Box/NN-TL)', 22),\n",
       " ('(CHUNK Grove/NN-TL)', 21),\n",
       " ('(CHUNK Horn/NN-TL)', 20),\n",
       " ('(CHUNK News/NN-TL)', 19),\n",
       " ('(CHUNK Park/NN-TL)', 18),\n",
       " ('(CHUNK King/NN-TL)', 17),\n",
       " ('(CHUNK Stone/NN-TL)', 16),\n",
       " ('(CHUNK Man/NN-TL)', 16),\n",
       " ('(CHUNK University/NN-TL)', 15),\n",
       " ('(CHUNK President/NN-TL)', 15),\n",
       " ('(CHUNK Homestead/NN-TL)', 14),\n",
       " ('(CHUNK Thing/NN-TL)', 14),\n",
       " ('(CHUNK Music/NN-TL)', 14),\n",
       " ('(CHUNK Center/NN-TL)', 14),\n",
       " ('(CHUNK College/NN-TL)', 14)]"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(23)"
   ]
  }
 ],
 "metadata": {
  "gist_id": "118d6388a385dd2fa712",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
